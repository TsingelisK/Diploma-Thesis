# -*- coding: utf-8 -*-
"""Clustering

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1I7oa3rzQ6m7OeIyeXeZzgVMsWMeNI698
"""

from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
import torch
import numpy as np
import pandas as pd
import os
import json
!pip install ijson
import ijson
!pip install transformers
from transformers import pipeline
from datetime import datetime as dt

from tqdm import tqdm
from sklearn.cluster import KMeans
import warnings
warnings.filterwarnings("ignore")

print('Loading train.json')
df_train=pd.read_json('./Twibot-20/train.json')
print('Loading test.json')
df_test=pd.read_json('./Twibot-20/test.json')
print('Loading support.json')
df_support=pd.read_json('./Twibot-20/support.json')
print('Loading dev.json')
df_dev=pd.read_json('./Twibot-20/dev.json')
print('Finished')
df_train=df_train.iloc[:,[0,1,2,3,5]]
df_test=df_test.iloc[:,[0,1,2,3,5]]
df_support=df_support.iloc[:,[0,1,2,3]]
df_dev=df_dev.iloc[:,[0,1,2,3,5]]
df_support['label']='None'
df_data_labeled=pd.concat([df_train,df_dev,df_test],ignore_index=True)
df_data=pd.concat([df_train,df_dev,df_test,df_support],ignore_index=True)

print('Building an array that contains all the tweets')
tweets=[]
for i in range (df_data.shape[0]):
    one_usr_tweets=[]
    if df_data['tweet'][i] is None:
       one_usr_tweets.append('')
    else:
       for each in df_data['tweet'][i]:
            one_usr_tweets.append(each)
    tweets.append(one_usr_tweets)
tweets=np.array(tweets)

print('Find the embedding for each tweet throgh pretarined NLP model RoBERTa')
feature_extract=pipeline('feature-extraction',model='roberta-base',tokenizer='roberta-base',device=0,padding=True, truncation=True,max_length=100, add_special_tokens = True)
one_user_tweets_emb=[]
every_tweet_embedding=[]
for each_person_tweets in tqdm(tweets):
    each_tweet_embedding=[]
    for j,each_tweet in enumerate(each_person_tweets):
        each_tweet_emb=torch.tensor(feature_extract(each_tweet))
        for k,each_word in enumerate(each_tweet_emb[0]):
            if k==0:
                total_word_tensor=each_word
            else:
                total_word_tensor+=each_word
            total_word_tensor/=each_tweet_emb.shape[1]
        every_tweet_embedding.append(total_word_tensor)

print('Start clustering')
correct_form=np.vstack(every_tweet_embedding)
range_n_clusters = [5,6,7,8,9,10,11,12,13,14,15]
silhouette_avg = []
for num_clusters in range_n_clusters:
    kmeans = KMeans(n_clusters=num_clusters)
    kmeans.fit(correct_form)
    cluster_labels = kmeans.labels_
    silhouette_avg.append(silhouette_score(correct_form, cluster_labels))
plt.plot(range_n_clusters,silhouette_avg,'bx-')
plt.xlabel('Values of ')
plt.ylabel('Silhouette score')
plt.title('Silhouette analysis For Optimal k')
plt.show()
k=silhouette_avg.index(max(silhouette_avg))+5
print('the optimal value of k',k)

