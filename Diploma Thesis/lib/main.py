# -*- coding: utf-8 -*-
"""Main

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EIzG_eEk5_MhUNHOAp3GNs2dHkZKvjRE
"""

from model import My_bot_detector
from Dataset import Twibot20
import torch
from torch import nn
import pickle
import numpy as np
from sklearn.metrics import f1_score,accuracy_score,precision_score,recall_score,matthews_corrcoef

def init_weights(m):
     if isinstance(m, nn.Linear):
         torch.nn.init.xavier_uniform_(m.weight)
         m.bias.data.fill_(0.0)


device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


dataset=Twibot20(device=device,process=False,save=True)
des_tensor,usr_embed_after_clustering['cluster0'],usr_embed_after_clustering['cluster1'],usr_embed_after_clustering['cluster2'],usr_embed_after_clustering['cluster3'],usr_embed_after_clustering['cluster4'],num_prop,category_prop,edge_index,edge_type,labels,train_idx,val_idx,test_idx=dataset.dataloader()

clus0=np.concatenate((emb1['cluster0'],emb2['cluster0'],emb3['cluster0'],emb4['cluster0'],emb5['cluster0']),axis=0)
lst0 = [torch.tensor(item).float() for item in clus0]
tweet0=torch.stack(lst0,0).to(device)

clus1=np.concatenate((emb1['cluster1'],emb2['cluster1'],emb3['cluster1'],emb4['cluster1'],emb5['cluster1']),axis=0)
lst1 = [torch.tensor(item).float() for item in clus1]
tweet1=torch.stack(lst1,0).to(device)

clus2=np.concatenate((emb1['cluster2'],emb2['cluster2'],emb3['cluster2'],emb4['cluster2'],emb5['cluster2']),axis=0)
lst2 = [torch.tensor(item).float() for item in clus2]
tweet2=torch.stack(lst2,0).to(device)

clus3=np.concatenate((emb1['cluster3'],emb2['cluster3'],emb3['cluster3'],emb4['cluster3'],emb5['cluster3']),axis=0)
lst3 = [torch.tensor(item).float() for item in clus3]
tweet3=torch.stack(lst3,0).to(device)

clus4=np.concatenate((emb1['cluster4'],emb2['cluster4'],emb3['cluster4'],emb4['cluster4'],emb5['cluster4']),axis=0)
lst4 = [torch.tensor(item).float() for item in clus4]
tweet4=torch.stack(lst4,0).to(device)

model=My_bot_detector(cluster_emb=128,des_emb=32,cat_emb=32,num_emb=32).to(device)
lr,weight_decay=1e-3,5e-3

loss=nn.CrossEntropyLoss()
optimizer = torch.optim.AdamW(model.parameters(),
                              lr=lr,weight_decay=weight_decay)

def train(epochs):
    validation=0
    for epoch in range(epochs):
        output = model(des_tensor,tweet0,tweet1,tweet2,tweet3,tweet4,num_properties,cat_properties,edge_index,edge_type)
        output1=output.max(1)[1].to('cpu').detach().numpy()
        labels1=labels.to('cpu').detach().numpy()

        loss_train = loss(output[train_idx], labels[train_idx])
        acc_train = accuracy_score(labels1[train_idx],output1[train_idx])
        acc_val = accuracy_score(labels1[val_idx],output1[val_idx])
        optimizer.zero_grad()
        loss_train.backward()
        optimizer.step()
        if acc_val>=validation:
           validation=acc_val
           with open('/content/drive/MyDrive/Twibot-20/model_pkl' , 'wb') as files:
                pickle.dump(model, files)

        print('Epoch: {:04d}'.format(epoch+1),
              'loss_train: {:.4f}'.format(loss_train.item()),
              'acc_train: {:.4f}'.format(acc_train),
              'acc_val: {:.4f}'.format(acc_val))
    return acc_train,loss_train

def test():
    output = model(des_tensor,tweet0,tweet1,tweet2,tweet3,tweet4,num_properties,cat_properties,edge_index,edge_type)
    #to output perilamvanei 2 times mia gia to class_bot kai mia gia to class_human
    #me thn entolh .....detach pernoyme enan nympy array apo enan tensor kai epipleon metatrepoyme ta labels se 0,1
    #0 an h timh gia to class bot einai megalyterh kai 1 an h timh gia to class_human einai megalyterh
    loss_test = loss(output[test_idx], labels[test_idx])
    output1=output.max(1)[1].to('cpu').detach().numpy()
    label1=labels.to('cpu').detach().numpy()
    acc_test = accuracy_score(label1[test_idx],output1[test_idx])
    precision_test=precision_score(label1[test_idx],output1[test_idx])
    recall_test=recall_score(label1[test_idx],output1[test_idx])
    f1=f1_score(label1[test_idx],output1[test_idx])
    mcc=matthews_corrcoef(label1[test_idx], output1[test_idx])
    print("Test set results:",
            "test_loss= {:.4f}".format(loss_test.item()),
            "test_accuracy= {:.4f}".format(acc_test.item()),
            "f1_score= {:.4f}".format(f1.item()),
            "recall_score= {:.4f}".format(recall_test.item()),
            "precission_score= {:.4f}".format(precision_test.item()),
            "mcc= {:.4f}".format(mcc.item()),
            )



train(150)
with open('/content/drive/MyDrive/Twibot-20/model_pkl' , 'rb') as f:
        model = pickle.load(f)

test()